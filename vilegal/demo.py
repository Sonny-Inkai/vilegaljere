#!/usr/bin/env python3
"""
Demo script for Vietnamese Legal Joint Entity-Relation Extraction
Usage: python demo.py --model_path /path/to/trained/model
"""

import os
import sys
import argparse
import torch
from transformers import AutoConfig, AutoModelForSeq2SeqLM, AutoTokenizer

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.utils import extract_vilegal_triplets, format_example_for_display

def load_model(model_path):
    """Load trained model and tokenizer"""
    print(f"ðŸ¤– Loading model from: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    config = AutoConfig.from_pretrained(model_path)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_path, config=config)
    
    model.eval()
    
    print(f"âœ… Model loaded successfully!")
    print(f"ðŸ”¤ Vocabulary size: {len(tokenizer)}")
    print(f"ðŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    return model, tokenizer

def predict(text, model, tokenizer, max_length=512, num_beams=3):
    """Generate prediction for input text"""
    
    # Tokenize input
    inputs = tokenizer(
        text,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    
    # Generate prediction
    with torch.no_grad():
        generated_tokens = model.generate(
            inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            max_length=max_length,
            num_beams=num_beams,
            early_stopping=False,
            length_penalty=0,
            use_cache=True
        )
    
    # Decode prediction
    prediction = tokenizer.decode(generated_tokens[0], skip_special_tokens=False)
    prediction = prediction.replace('<pad>', '').replace('</s>', '').replace('<s>', '').strip()
    
    return prediction

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", required=True, type=str, help="Path to trained model")
    parser.add_argument("--max_length", default=512, type=int)
    parser.add_argument("--num_beams", default=3, type=int)
    args = parser.parse_args()
    
    # Load model
    model, tokenizer = load_model(args.model_path)
    
    print("\nðŸš€ Vietnamese Legal Joint Entity-Relation Extraction Demo")
    print("="*80)
    print("ðŸ“ Enter Vietnamese legal text to extract entities and relations")
    print("ðŸ’¡ Type 'quit' or 'exit' to stop")
    print("="*80)
    
    # Interactive demo
    while True:
        print("\nðŸ“„ Enter legal text:")
        text = input("> ").strip()
        
        if text.lower() in ['quit', 'exit', 'q']:
            print("ðŸ‘‹ Goodbye!")
            break
        
        if not text:
            print("âš ï¸ Please enter some text")
            continue
        
        print("\nðŸ”® Generating prediction...")
        
        try:
            prediction = predict(text, model, tokenizer, args.max_length, args.num_beams)
            
            print("\n" + "="*80)
            print("ðŸ“„ INPUT:")
            print(text)
            print("\nðŸŽ¯ PREDICTED OUTPUT:")
            print(prediction)
            
            # Extract and display triplets
            triplets = extract_vilegal_triplets(prediction)
            print(f"\nðŸ“Š EXTRACTED TRIPLETS ({len(triplets)}):")
            if triplets:
                for i, (head_type, head_text, tail_type, tail_text, relation) in enumerate(triplets, 1):
                    print(f"  {i}. {head_type}: '{head_text}' --[{relation}]--> {tail_type}: '{tail_text}'")
            else:
                print("  No triplets extracted")
            
            print("="*80)
            
        except Exception as e:
            print(f"âŒ Error during prediction: {str(e)}")

# Sample examples for testing
SAMPLE_EXAMPLES = [
    "Äiá»u 51: Tham gia cá»§a nhÃ  Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i, tá»• chá»©c kinh táº¿ cÃ³ vá»‘n Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i trÃªn thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam 1. NhÃ  Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i, tá»• chá»©c kinh táº¿ cÃ³ vá»‘n Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i khi tham gia Ä‘áº§u tÆ°, hoáº¡t Ä‘á»™ng trÃªn thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam tuÃ¢n thá»§ quy Ä‘á»‹nh vá» tá»· lá»‡ sá»Ÿ há»¯u nÆ°á»›c ngoÃ i, Ä‘iá»u kiá»‡n, trÃ¬nh tá»±, thá»§ tá»¥c Ä‘áº§u tÆ° theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t vá» chá»©ng khoÃ¡n vÃ  thá»‹ trÆ°á»ng chá»©ng khoÃ¡n.",
    
    "ChÃ­nh phá»§ quy Ä‘á»‹nh chi tiáº¿t tá»· lá»‡ sá»Ÿ há»¯u nÆ°á»›c ngoÃ i, Ä‘iá»u kiá»‡n, trÃ¬nh tá»±, thá»§ tá»¥c Ä‘áº§u tÆ°, viá»‡c tham gia cá»§a nhÃ  Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i, tá»• chá»©c kinh táº¿ cÃ³ vá»‘n Ä‘áº§u tÆ° nÆ°á»›c ngoÃ i trÃªn thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam.",
    
    "Theo Luáº­t Doanh nghiá»‡p 2020, doanh nghiá»‡p cÃ³ trÃ¡ch nhiá»‡m tuÃ¢n thá»§ cÃ¡c quy Ä‘á»‹nh vá» báº£o vá»‡ mÃ´i trÆ°á»ng vÃ  phÃ¡t triá»ƒn bá»n vá»¯ng."
]

def demo_with_samples():
    """Run demo with sample texts"""
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", required=True, type=str, help="Path to trained model")
    parser.add_argument("--max_length", default=512, type=int)
    parser.add_argument("--num_beams", default=3, type=int)
    args = parser.parse_args()
    
    # Load model
    model, tokenizer = load_model(args.model_path)
    
    print("\nðŸŽ¯ Running demo with sample Vietnamese legal texts")
    print("="*80)
    
    for i, text in enumerate(SAMPLE_EXAMPLES, 1):
        print(f"\n--- SAMPLE {i} ---")
        
        try:
            prediction = predict(text, model, tokenizer, args.max_length, args.num_beams)
            format_example_for_display(text, "", prediction)
            
            input("\nPress Enter to continue to next sample...")
            
        except Exception as e:
            print(f"âŒ Error during prediction: {str(e)}")
    
    print("\nâœ… Demo completed!")

if __name__ == "__main__":
    if len(sys.argv) > 1 and "--samples" in sys.argv:
        sys.argv.remove("--samples")
        demo_with_samples()
    else:
        main() 