import os
from huggingface_hub import HfApi, HfFolder, login, create_repo
from transformers import AutoTokenizer

def upload_tokenizer_to_hf_hub(local_tokenizer_dir: str, repo_id: str, hf_token: str = None, is_private: bool = False):
    """
    Táº£i tokenizer tá»« thÆ° má»¥c cá»¥c bá»™ vÃ  Ä‘Äƒng lÃªn Hugging Face Hub.

    Args:
        local_tokenizer_dir (str): ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a tokenizer Ä‘Ã£ huáº¥n luyá»‡n cá»¥c bá»™.
        repo_id (str): ID cá»§a repository trÃªn Hugging Face Hub (vÃ­ dá»¥: "username/tokenizer-name").
        hf_token (str, optional): Token API cá»§a Hugging Face. Náº¿u lÃ  None, sáº½ cá»‘ gáº¯ng sá»­ dá»¥ng token Ä‘Ã£ lÆ°u
                                   hoáº·c nháº¯c ngÆ°á»i dÃ¹ng Ä‘Äƒng nháº­p.
        is_private (bool, optional): Äáº·t repository lÃ  private hay public. Máº·c Ä‘á»‹nh lÃ  False (public).
    """
    print(f"Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh Ä‘Äƒng tokenizer tá»« '{local_tokenizer_dir}' lÃªn '{repo_id}'...")

    # --- 1. Kiá»ƒm tra thÆ° má»¥c tokenizer cá»¥c bá»™ ---
    if not os.path.isdir(local_tokenizer_dir):
        print(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c tokenizer cá»¥c bá»™ táº¡i '{local_tokenizer_dir}'.")
        print("Vui lÃ²ng kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n.")
        return

    # --- 2. ÄÄƒng nháº­p vÃ o Hugging Face Hub (náº¿u cáº§n) ---
    # Æ¯u tiÃªn token Ä‘Æ°á»£c truyá»n vÃ o, sau Ä‘Ã³ lÃ  token Ä‘Ã£ lÆ°u, cuá»‘i cÃ¹ng lÃ  nháº¯c Ä‘Äƒng nháº­p.
    token_to_use = hf_token
    if not token_to_use:
        token_to_use = HfFolder.get_token() # Láº¥y token Ä‘Ã£ lÆ°u náº¿u cÃ³

    if not token_to_use:
        print("\nKhÃ´ng tÃ¬m tháº¥y token Hugging Face Ä‘Ã£ lÆ°u.")
        print("Äang cá»‘ gáº¯ng Ä‘Äƒng nháº­p. Vui lÃ²ng lÃ m theo hÆ°á»›ng dáº«n (cÃ³ thá»ƒ má»Ÿ trÃ¬nh duyá»‡t hoáº·c yÃªu cáº§u nháº­p token).")
        try:
            login() # Sáº½ nháº¯c ngÆ°á»i dÃ¹ng cung cáº¥p token náº¿u chÆ°a Ä‘Äƒng nháº­p
            token_to_use = HfFolder.get_token()
            if not token_to_use:
                print("ÄÄƒng nháº­p khÃ´ng thÃ nh cÃ´ng hoáº·c khÃ´ng láº¥y Ä‘Æ°á»£c token. Vui lÃ²ng thá»­ láº¡i.")
                return
            print("ÄÄƒng nháº­p thÃ nh cÃ´ng!")
        except Exception as e:
            print(f"Lá»—i trong quÃ¡ trÃ¬nh Ä‘Äƒng nháº­p: {e}")
            print("HÃ£y thá»­ Ä‘Äƒng nháº­p thá»§ cÃ´ng báº±ng 'huggingface-cli login' trong terminal rá»“i cháº¡y láº¡i script.")
            return
    else:
        print("ÄÃ£ tÃ¬m tháº¥y token Hugging Face.")

    # --- 3. Táº¡o repository trÃªn Hugging Face Hub (náº¿u chÆ°a cÃ³) ---
    # Sá»­ dá»¥ng HfApi Ä‘á»ƒ cÃ³ nhiá»u quyá»n kiá»ƒm soÃ¡t hÆ¡n
    api = HfApi(token=token_to_use)
    try:
        # Kiá»ƒm tra xem repo Ä‘Ã£ tá»“n táº¡i chÆ°a
        try:
            api.repo_info(repo_id=repo_id)
            print(f"Repository '{repo_id}' Ä‘Ã£ tá»“n táº¡i trÃªn Hugging Face Hub.")
        except Exception: # RepoResourceNotFound or other errors
            print(f"Repository '{repo_id}' chÆ°a tá»“n táº¡i. Äang táº¡o má»›i...")
            create_repo(repo_id, token=token_to_use, private=is_private, repo_type="model", exist_ok=True)
            print(f"ÄÃ£ táº¡o thÃ nh cÃ´ng repository '{repo_id}'.")
    except Exception as e:
        print(f"Lá»—i khi táº¡o hoáº·c kiá»ƒm tra repository '{repo_id}': {e}")
        return

    # --- 4. Táº£i tokenizer cá»¥c bá»™ ---
    print(f"\nÄang táº£i tokenizer tá»« thÆ° má»¥c cá»¥c bá»™: '{local_tokenizer_dir}'...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_dir)
        print("Táº£i tokenizer cá»¥c bá»™ thÃ nh cÃ´ng.")
    except Exception as e:
        print(f"Lá»—i khi táº£i tokenizer tá»« '{local_tokenizer_dir}': {e}")
        print("HÃ£y Ä‘áº£m báº£o thÆ° má»¥c chá»©a cÃ¡c file tokenizer há»£p lá»‡ (tokenizer_config.json, .model, special_tokens_map.json).")
        return

    # --- 5. Äáº©y tokenizer lÃªn Hugging Face Hub ---
    print(f"\nÄang Ä‘áº©y tokenizer lÃªn repository '{repo_id}'...")
    try:
        # push_to_hub sáº½ tá»± Ä‘á»™ng táº£i lÃªn cÃ¡c file cáº§n thiáº¿t tá»« thÆ° má»¥c tokenizer
        # bao gá»“m cáº£ file .model cá»§a SentencePiece vÃ  cÃ¡c file JSON cáº¥u hÃ¬nh.
        tokenizer.push_to_hub(
            repo_id=repo_id,
            commit_message="Upload tokenizer files",
            private=is_private,
            token=token_to_use, # Äáº£m báº£o sá»­ dá»¥ng token Ä‘Ã£ xÃ¡c thá»±c
            # create_repo=True # ÄÃ£ xá»­ lÃ½ á»Ÿ trÃªn, nhÆ°ng Ä‘á»ƒ Ä‘Ã¢y cÅ©ng khÃ´ng sao
        )
        hub_url = f"https://huggingface.co/{repo_id}"
        print("\n--------------------------------------------------------------------")
        print("ğŸ‰ ÄÄ‚NG TOKENIZER LÃŠN HUGGING FACE HUB THÃ€NH CÃ”NG! ğŸ‰")
        print("--------------------------------------------------------------------")
        print(f"ChÃ¡u cÃ³ thá»ƒ xem tokenizer cá»§a mÃ¬nh táº¡i: {hub_url}")
        print("\nÄá»ƒ sá»­ dá»¥ng tokenizer nÃ y trong cÃ¡c dá»± Ã¡n khÃ¡c, hÃ£y dÃ¹ng:")
        print("from transformers import AutoTokenizer")
        print(f"tokenizer = AutoTokenizer.from_pretrained('{repo_id}')")
        print("--------------------------------------------------------------------")

    except Exception as e:
        print(f"Lá»—i khi Ä‘áº©y tokenizer lÃªn Hugging Face Hub: {e}")
        print("Má»™t sá»‘ nguyÃªn nhÃ¢n phá»• biáº¿n:")
        print("- Token khÃ´ng cÃ³ quyá»n 'write'.")
        print("- Váº¥n Ä‘á» vá» káº¿t ná»‘i máº¡ng.")
        print("- Repository cÃ³ thá»ƒ Ä‘Ã£ cÃ³ file vá»›i tÃªn tÆ°Æ¡ng tá»± vÃ  gÃ¢y xung Ä‘á»™t (Ã­t gáº·p vá»›i tokenizer).")

if __name__ == "__main__":
    print("--- SCRIPT ÄÄ‚NG TOKENIZER LÃŠN HUGGING FACE HUB ---")

    # Láº¥y thÃ´ng tin tá»« ngÆ°á»i dÃ¹ng
    default_local_dir = "./vietnamese_legal_hf_tokenizer"
    local_dir_input = input(f"Nháº­p Ä‘Æ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c tokenizer cá»¥c bá»™ cá»§a chÃ¡u (máº·c Ä‘á»‹nh: {default_local_dir}): ")
    local_tokenizer_dir = local_dir_input if local_dir_input else default_local_dir

    # HÆ°á»›ng dáº«n vá» repo_id
    print("\nLÆ°u Ã½ vá» 'repo_id':")
    print("ÄÃ¢y lÃ  tÃªn Ä‘á»‹nh danh cho tokenizer cá»§a chÃ¡u trÃªn Hugging Face Hub.")
    print("NÃ³ thÆ°á»ng cÃ³ dáº¡ng: 'ten_username_hf_cua_chau/ten_tokenizer_mong_muon'")
    print("VÃ­ dá»¥: 'john_doe/vietnamese-legal-tokenizer'")
    repo_id_input = input("Nháº­p repo_id chÃ¡u muá»‘n sá»­ dá»¥ng trÃªn Hugging Face Hub: ")
    if not repo_id_input:
        print("Lá»—i: repo_id khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ trá»‘ng.")
        exit()

    private_input = input("ChÃ¡u cÃ³ muá»‘n Ä‘áº·t tokenizer nÃ y á»Ÿ cháº¿ Ä‘á»™ riÃªng tÆ° (private) khÃ´ng? (yes/no, máº·c Ä‘á»‹nh: no): ").lower()
    is_private_repo = private_input == 'yes'

    # KhÃ´ng yÃªu cáº§u token trá»±c tiáº¿p trong script Ä‘á»ƒ tÄƒng tÃ­nh báº£o máº­t.
    # Khuyáº¿n khÃ­ch ngÆ°á»i dÃ¹ng Ä‘Äƒng nháº­p qua `huggingface-cli login` trÆ°á»›c.
    # Hoáº·c hÃ m login() sáº½ tá»± xá»­ lÃ½.
    print("\nScript sáº½ cá»‘ gáº¯ng sá»­ dá»¥ng token Hugging Face Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trÃªn mÃ¡y cá»§a chÃ¡u.")
    print("Náº¿u chÆ°a Ä‘Äƒng nháº­p, má»™t lá»i nháº¯c (cÃ³ thá»ƒ má»Ÿ trÃ¬nh duyá»‡t) sáº½ xuáº¥t hiá»‡n Ä‘á»ƒ chÃ¡u cung cáº¥p token.")

    upload_tokenizer_to_hf_hub(local_tokenizer_dir, repo_id_input, is_private=is_private_repo)
